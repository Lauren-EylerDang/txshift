utils::globalVariables(c(":=", "no_records_this_obs"))
################################################################################

#' Conditional density estimation with HAL in a single cross-validation fold
#'
#' @param fold Object specifying cross-validation folds as generated by a call
#'  to \code{origami::make_folds}.
#' @param long_data A \code{data.table} or \code{data.frame} object containing
#'  the data in the long format discussed in Diaz & van der Laan (2011, IJB),
#'  as produced by \code{\link{format_long_hazards}}.
#' @param wts A \code{numeric} vector of observation-level weights, matching in
#'  its length the number of records present in the long format data. The
#'  default is to weight all observations equally.
#' @param lambda_seq A \code{numeric} sequence of values of the lambda tuning
#'  parameter of the Lasso L1 regression, to be passed to \code{glmnet::glmnet}
#'  through a call to \code{hal9001::fit_hal}.
#'
#' @importFrom stats aggregate plogis
#' @importFrom origami training validation fold_index
#' @importFrom future.apply future_lapply
#' @importFrom assertthat assert_that
#' @importFrom hal9001 fit_hal
#
cv_haldensify <- function(fold, long_data, wts = rep(1, nrow(long_data)),
                          lambda_seq = exp(seq(-1, -13, length = 1000))) {
  # make training and validation folds
  train_set <- origami::training(long_data)
  valid_set <- origami::validation(long_data)

  # subset observation-level weights to the correct size
  wts_train <- wts[fold$training_set]
  wts_valid <- wts[fold$validation_set]

  # fit a HAL regression on the training set
  # NOTE: pass IDs to glmnet?
  hal_fit_train <- hal9001::fit_hal(
    X = as.matrix(train_set[, -c(1, 2)]),
    Y = as.numeric(train_set$in_bin),
    fit_type = "glmnet",
    use_min = TRUE,
    family = "binomial",
    return_lasso = TRUE,
    lambda = lambda_seq,
    fit_glmnet = TRUE,
    standardize = FALSE, # pass to glmnet
    weights = wts_train, # pass to glmnet
    yolo = FALSE
  )

  # get intercept and coefficient fits for this value of lambda from glmnet
  alpha_hat <- hal_fit_train$glmnet_lasso$a0
  betas_hat <- hal_fit_train$glmnet_lasso$beta
  coefs_hat <- rbind(alpha_hat, betas_hat)

  # make design matrix for validation set manually
  pred_x_basis <- hal9001:::make_design_matrix(
    as.matrix(valid_set[, -c(1, 2)]),
    hal_fit_train$basis_list
  )
  pred_x_basis <- hal9001:::apply_copy_map(
    pred_x_basis,
    hal_fit_train$copy_map
  )
  pred_x_basis <- cbind(rep(1, nrow(valid_set)), pred_x_basis)

  # manually predict along sequence of lambdas
  preds_logit <- pred_x_basis %*% coefs_hat
  preds <- stats::plogis(as.matrix(preds_logit))

  # compute hazard for a given observation by looping over individuals
  hazards_pred_each_obs <-
    future.apply::future_lapply(unique(valid_set$obs_id), function(id) {
      # get predictions for the current observation only
      preds_this_obs <- matrix(preds[valid_set$obs_id == id, ],
        ncol = length(lambda_seq)
      )
      n_records_this_obs <- nrow(preds_this_obs)

      # NOTE: pred_hazard = (1 - pred) if 0 in this bin * pred if 1 in this bin
      if (n_records_this_obs > 1) {
        hazard_prefailure <- (1 - preds_this_obs[-n_records_this_obs, ])
        hazard_at_failure <- preds_this_obs[n_records_this_obs, ]
        hazards_pred <- rbind(hazard_prefailure, hazard_at_failure)
      } else {
        hazards_pred <- preds_this_obs
      }

      # check dimensions
      assertthat::assert_that(all(dim(preds_this_obs) == dim(hazards_pred)))

      # multiply hazards across rows to construct the individual-level hazard
      hazards_pred <- matrix(apply(hazards_pred, 2, prod), nrow = 1)
      return(hazards_pred)
  })

  # aggregate predictions across observations
  hazards_pred <- do.call(rbind, hazards_pred_each_obs)

  # collapse weights to the observation level
  wts_valid_reduced <- stats::aggregate(
    wts_valid, list(valid_set$obs_id),
    unique
  )
  colnames(wts_valid_reduced) <- c("id", "weight")

  # construct output
  out <- list(
    preds = hazards_pred,
    ids = wts_valid_reduced$id,
    wts = wts_valid_reduced$weight,
    fold = origami::fold_index()
  )
  return(out)
}

################################################################################

#' Cross-validated conditional density estimation with HAL
#'
#' @param A The \code{numeric} vector or similar of the observed values of an
#'  intervention for a group of observational units of interest.
#' @param W A \code{data.frame}, \code{matrix}, or similar giving the values of
#'  baseline covariates (potential confounders) for the observed units whose
#'  observed intervention values are provided in the previous argument.
#' @param wts A \code{numeric} vector of observation-level weights. The default
#'  is to weight all observations equally.
#' @param grid_type A \code{character} indicating the strategy to be used in
#'  creating bins along the observed support of the intervention \code{A}. For
#'  bins of equal range, use "equal_range", consulting the documentation of
#'  \code{ggplot2::cut_interval} for more information. To ensure each bins has
#'  the same number of points, use "equal_mass" and consult the documentation of
#'  \code{ggplot2::cut_number} for details. If bins of equal width are desired,
#'  use "equal_width" and consult \code{ggplot2::cut_width} as a reference.
#' @param n_bins Only used if \code{type} is set to \code{"equal_range"} or
#'  \code{"equal_mass"}. This \code{numeric} value indicates the number of bins
#'  that the support of the intervention \code{A} is to be divided into.
#' @param width Only used if \code{type} is set to \code{"equal_width"}. This
#'  \code{numeric} provides the width of the bins to be produced along the
#'  support of the observed values of the intervention \code{A}.
#' @param lambda_seq A \code{numeric} sequence of values of the lambda tuning
#'  parameter of the Lasso L1 regression, to be passed to \code{glmnet::glmnet}
#'  through a call to \code{hal9001::fit_hal}.
#'
#' @importFrom origami make_folds cross_validate
#' @importFrom hal9001 fit_hal
#'
#' @export
#
haldensify <- function(A, W, wts = rep(1, length(A)),
                       grid_type = c(
                         "equal_range", "equal_mass",
                         "equal_width"
                       ),
                       n_bins = 10, width = NULL,
                       lambda_seq = exp(seq(-1, -13, length = 1000))) {
  # catch input
  call <- match.call(expand.dots = TRUE)

  # re-format input data into long hazards structure
  long_data <- format_long_hazards(A = A, W = W, wts = wts,
                                   type = grid_type, n_bins = n_bins,
                                   width = width)

  # extract weights from long format data structure
  wts_long <- long_data$wts
  long_data[, wts := NULL]

  # make folds with origami
  folds <- origami::make_folds(long_data, cluster_ids = long_data$obs_id)

  # call cross_validate on cv_density function...
  haldensity <- origami::cross_validate(
    cv_fun = cv_haldensify,
    folds = folds,
    long_data = long_data,
    wts = wts_long,
    lambda_seq = lambda_seq,
    use_future = FALSE,
    .combine = FALSE
  )

  # re-organize output from origami::cross_validate
  hazards_pred <- do.call(rbind, haldensity$preds)
  obs_ids <- do.call(c, haldensity$ids)
  obs_wts <- do.call(c, haldensity$wts)

  # compute loss for the given individual
  hazards_loss <- apply(hazards_pred, 2, function(x) {
    pred_weighted <- x * obs_wts
    loss_weighted <- -log(pred_weighted)
    return(loss_weighted)
  })

  # take column means to have average loss across sequence of lambdas
  loss_mean <- colMeans(hazards_loss)
  lambda_loss_min <- lambda_seq[which.min(loss_mean)]

  # fit a HAL regression on the full data set with the CV-selected lambda
  hal_fit <- hal9001::fit_hal(
    X = as.matrix(long_data[, -c(1, 2)]),
    Y = as.numeric(long_data$in_bin),
    fit_type = "glmnet",
    use_min = TRUE,
    family = "binomial",
    return_lasso = TRUE,
    lambda = lambda_loss_min,
    fit_glmnet = TRUE,
    standardize = FALSE, # pass to glmnet
    weights = wts_long,  # pass to glmnet
    yolo = FALSE
  )

  # construct output
  out <- list(
    hal_fit = hal_fit,
    call = call
  )
  class(out) <- "haldensify"
  return(out)
}

################################################################################

#' Prediction method for HAL-based conditional density estimation
#'
#' @param object An object of class \code{haldensify}, containing the results of
#'  fitting the highly adaptive lasso for conditional density estimation, as
#'  produced by a call to \code{haldensify}.
#' @param ... Additional arguments passed to \code{predict} as necessary.
#' @param new_A The \code{numeric} vector or similar of the observed values of
#'  an intervention for a group of observational units of interest.
#' @param new_W A \code{data.frame}, \code{matrix}, or similar giving the values
#'  of baseline covariates (potential confounders) for the observed units whose
#'  observed intervention values are provided in the previous argument.
#' @param wts A \code{numeric} vector of observation-level weights. The default
#'  is to weight all observations equally.
#'
#' @importFrom stats aggregate
#'
#' @export
#
predict.haldensify <- function(object, ..., new_A, new_W,
                               wts = rep(1, length(new_A))) {
  # make long format data structure with new input data
  long_format_args <- list(A = new_A,
                           W = new_W,
                           wts = wts,
                           type = object$call$grid_type,
                           n_bins = object$call$n_bins,
                           width = object$call$n_bins)
  long_data <- do.call(format_long_hazards, long_format_args)

  # predict conditional density estimate from HAL fit on new long format data
  density_pred <- predict(object$hal_fit, new_data = long_data)

  # NOTE: something seems missing here?
  density_pred_reduced <- stats::aggregate(
    density_pred, list(long_data$obs_id),
    prod
  )
  colnames(density_pred_reduced) <- c("id", "density")

  # output
  return(density_pred_reduced$density)
}

################################################################################

#' Generate long format hazards data for conditional density estimation
#'
#' @param A The \code{numeric} vector or similar of the observed values of an
#'  intervention for a group of observational units of interest.
#' @param W A \code{data.frame}, \code{matrix}, or similar giving the values of
#'  baseline covariates (potential confounders) for the observed units whose
#'  observed intervention values are provided in the previous argument.
#' @param wts A \code{numeric} vector of observation-level weights. The default
#'  is to weight all observations equally.
#' @param type A \code{character} indicating the strategy to be used in creating
#'  bins along the observed support of the intervention \code{A}. For bins of
#'  equal range, use "equal_range" and consider consulting the documentation of
#'  \code{ggplot2::cut_interval} for more information. To ensure each bins has
#'  the same number of points, use "equal_mass" and consult the documentation of
#'  \code{ggplot2::cut_number} for details. If bins of equal width are desired,
#'  use "equal_width" and consult \code{ggplot2::cut_width} as a reference.
#' @param n_bins Only used if \code{type} is set to \code{"equal_range"} or
#'  \code{"equal_mass"}. This \code{numeric} value indicates the number of bins
#'  that the support of the intervention \code{A} is to be divided into.
#' @param width Only used if \code{type} is set to \code{"equal_width"}. This
#'  \code{numeric} provides the width of the bins to be produced along the
#'  support of the observed values of the intervention \code{A}.
#'
#' @importFrom data.table as.data.table setnames
#' @importFrom ggplot2 cut_interval cut_number cut_width
#' @importFrom future.apply future_lapply
#' @importFrom assertthat assert_that
#
format_long_hazards <- function(A, W, wts = rep(1, length(A)),
                                type = c(
                                  "equal_range", "equal_mass",
                                  "equal_width"
                                ),
                                n_bins = 10, width = NULL) {
  # clean up arguments
  type <- match.arg(type)

  # set grid along A and find interval membership of observations along grid
  if (type == "equal_range") {
    bins <- ggplot2::cut_interval(A, n_bins, right = FALSE)
  } else if (type == "equal_mass") {
    bins <- ggplot2::cut_number(A, n_bins, right = FALSE)
  } else {
    assertthat::assert_that(!is.null(width))
    bins <- ggplot2::cut_width(A, width, closed = "left")
  }
  bin_id <- as.numeric(bins)

  # loop over observations to create expanded set of records for each
  reformat_each_obs <- future.apply::future_lapply(seq_along(A), function(i) {
    # create repeating bin IDs for this subject (these map to intervals)
    all_bins <- matrix(seq_along(levels(bins)), ncol = 1)

    # create indicator and "turn on" indicator for interval membership
    bin_indicator <- rep(0, nrow(all_bins))
    bin_indicator[bin_id[i]] <- 1
    id <- rep(i, nrow(all_bins))

    # get correct value of baseline variables and repeat along intervals
    if (is.null(dim(W))) {
      # assume vector
      obs_w <- rep(W[i], nrow(all_bins))
      names_w <- "W"
    } else {
      # assume two-dimensional array
      obs_w <- rep(as.numeric(W[i, ]), nrow(all_bins))
      obs_w <- matrix(obs_w, ncol = ncol(W), byrow = TRUE)

      # use names from array if present
      if (is.null(names(W))) {
        names_w <- paste("W", seq_len(ncol(W)), sep = "_")
      } else {
        names_w <- names(W)
      }
    }

    # get correct value of weights and repeat along intervals
    # NOTE: the weights are always a vector
    obs_wts <- rep(wts[i], nrow(all_bins))

    # create data table with membership indicator and interval limits
    suppressWarnings(
      hazards_df <- data.table::as.data.table(cbind(
        id, bin_indicator,
        all_bins, obs_w,
        obs_wts
      ))
    )

    # trim records to simply end at the failure time for a given observation
    hazards_df_reduced <- hazards_df[seq_len(bin_id[i]), ]

    # give explicit names and add to appropriate position in list
    hazards_df <-
      data.table::setnames(
        hazards_df_reduced,
        c("obs_id", "in_bin", "bin_id", names_w, "wts")
      )
    return(hazards_df)
  })

  # combine observation-level hazards data into larger structure
  out <- do.call(rbind, reformat_each_obs)
  return(out)
}

